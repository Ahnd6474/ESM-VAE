{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fac251f0",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-09-05T13:42:59.645835Z",
     "iopub.status.busy": "2025-09-05T13:42:59.645556Z",
     "iopub.status.idle": "2025-09-05T13:43:09.528709Z",
     "shell.execute_reply": "2025-09-05T13:43:09.527927Z"
    },
    "papermill": {
     "duration": 9.88805,
     "end_time": "2025-09-05T13:43:09.530220",
     "exception": false,
     "start_time": "2025-09-05T13:42:59.642170",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fair-esm\r\n",
      "  Downloading fair_esm-2.0.0-py3-none-any.whl.metadata (37 kB)\r\n",
      "Downloading fair_esm-2.0.0-py3-none-any.whl (93 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.1/93.1 kB\u001b[0m \u001b[31m921.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: fair-esm\r\n",
      "Successfully installed fair-esm-2.0.0\r\n",
      "Collecting biopython\r\n",
      "  Downloading biopython-1.85-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\r\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from biopython) (1.26.4)\r\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->biopython) (1.3.8)\r\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->biopython) (1.2.4)\r\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->biopython) (0.1.1)\r\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->biopython) (2025.2.0)\r\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->biopython) (2022.2.0)\r\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->biopython) (2.4.1)\r\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->biopython) (2024.2.0)\r\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->biopython) (2022.2.0)\r\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->biopython) (1.4.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->biopython) (2024.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->biopython) (2024.2.0)\r\n",
      "Downloading biopython-1.85-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: biopython\r\n",
      "Successfully installed biopython-1.85\r\n"
     ]
    }
   ],
   "source": [
    "!pip install fair-esm\n",
    "!pip install biopython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "806298c4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-05T13:43:09.537024Z",
     "iopub.status.busy": "2025-09-05T13:43:09.536774Z",
     "iopub.status.idle": "2025-09-05T13:43:15.520097Z",
     "shell.execute_reply": "2025-09-05T13:43:15.519270Z"
    },
    "papermill": {
     "duration": 5.987887,
     "end_time": "2025-09-05T13:43:15.521267",
     "exception": false,
     "start_time": "2025-09-05T13:43:09.533380",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000\n"
     ]
    }
   ],
   "source": [
    "from Bio import SeqIO\n",
    "seq_path='/kaggle/input/uniref50-sub/uniref50_subsample.fasta'\n",
    "sequences=[]\n",
    "for seq_record in SeqIO.parse(seq_path, \"fasta\"):\n",
    "    sequences.append(str(seq_record.seq))\n",
    "print(len(sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2d89194",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-05T13:43:15.528398Z",
     "iopub.status.busy": "2025-09-05T13:43:15.528163Z",
     "iopub.status.idle": "2025-09-05T18:30:50.589465Z",
     "shell.execute_reply": "2025-09-05T18:30:50.588422Z"
    },
    "papermill": {
     "duration": 17255.066918,
     "end_time": "2025-09-05T18:30:50.591037",
     "exception": false,
     "start_time": "2025-09-05T13:43:15.524119",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://dl.fbaipublicfiles.com/fair-esm/models/esm2_t33_650M_UR50D.pt\" to /root/.cache/torch/hub/checkpoints/esm2_t33_650M_UR50D.pt\n",
      "Downloading: \"https://dl.fbaipublicfiles.com/fair-esm/regression/esm2_t33_650M_UR50D-contact-regression.pt\" to /root/.cache/torch/hub/checkpoints/esm2_t33_650M_UR50D-contact-regression.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAE] loaded from 'model_sd'  missing=0  unexpected=0\n",
      "[SUR] initialized small surrogate (2L,4H,256D)\n",
      "[ADAPTER] Identity (no params)\n",
      "[save] VAEWithSurrogateBundle → /kaggle/working/vae_sur_leaky_init.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Smoke|eval] ENC vs SUR (TF):   0%|          | 0/2 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:508: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. We recommend specifying layout=torch.jagged when constructing a nested tensor, as this layout receives active development, has better operator coverage, and works with torch.compile. (Triggered internally at /pytorch/aten/src/ATen/NestedTensorImpl.cpp:178.)\n",
      "  output = torch._nested_tensor_from_mask(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py:5962: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py:5962: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n",
      "[Smoke|eval] ENC vs SUR (TF): 100%|██████████| 2/2 [00:03<00:00,  1.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[shapes] x=(8, 512)  h_enc=(8, 512, 256)  sur_mem=(8, 512, 256)\n",
      "[ENC-TF]  CE=0.0875  PPL=1.09  ACC@1=96.75%\n",
      "[SUR-TF]  CE=6.6109  PPL=743.15  ACC@1=7.94%\n",
      "[Δ SUR-ENC] ΔCE=+6.5234  ΔACC=-88.81%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] trainable params = 1,515,008\n",
      "[info] frozen    params = 5,756,961\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train(leaky enc-mem) ep=1/4]: 100%|██████████| 7031/7031 [1:08:10<00:00,  1.72it/s, loss=0.161, ce=0.1021, mse=0.009, cos=0.003]\n",
      "[Val(leaky)]: 100%|██████████| 782/782 [02:53<00:00,  4.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ep 1] train: loss=0.989 ce=0.0982 mse=0.130 cos=0.048 | valid: loss=0.334 ce=0.0843 mse=0.038 cos=0.012\n",
      "[save] VAEWithSurrogateBundle → /kaggle/working/vae_sur_leaky_best.pt\n",
      "[SAVE] best → /kaggle/working/vae_sur_leaky_best.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train(leaky enc-mem) ep=2/4]: 100%|██████████| 7031/7031 [1:08:28<00:00,  1.71it/s, loss=0.149, ce=0.0997, mse=0.007, cos=0.002]\n",
      "[Val(leaky)]: 100%|██████████| 782/782 [02:54<00:00,  4.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ep 2] train: loss=0.151 ce=0.0982 mse=0.008 cos=0.003 | valid: loss=0.130 ce=0.0843 mse=0.007 cos=0.002\n",
      "[save] VAEWithSurrogateBundle → /kaggle/working/vae_sur_leaky_best.pt\n",
      "[SAVE] best → /kaggle/working/vae_sur_leaky_best.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train(leaky enc-mem) ep=3/4]: 100%|██████████| 7031/7031 [1:08:26<00:00,  1.71it/s, loss=0.143, ce=0.0963, mse=0.007, cos=0.002]\n",
      "[Val(leaky)]: 100%|██████████| 782/782 [02:53<00:00,  4.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ep 3] train: loss=0.146 ce=0.0982 mse=0.007 cos=0.002 | valid: loss=0.129 ce=0.0843 mse=0.007 cos=0.002\n",
      "[save] VAEWithSurrogateBundle → /kaggle/working/vae_sur_leaky_best.pt\n",
      "[SAVE] best → /kaggle/working/vae_sur_leaky_best.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train(leaky enc-mem) ep=4/4]: 100%|██████████| 7031/7031 [1:08:23<00:00,  1.71it/s, loss=0.153, ce=0.1056, mse=0.007, cos=0.002]\n",
      "[Val(leaky)]: 100%|██████████| 782/782 [02:53<00:00,  4.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ep 4] train: loss=0.145 ce=0.0982 mse=0.007 cos=0.002 | valid: loss=0.128 ce=0.0843 mse=0.007 cos=0.002\n",
      "[save] VAEWithSurrogateBundle → /kaggle/working/vae_sur_leaky_best.pt\n",
      "[SAVE] best → /kaggle/working/vae_sur_leaky_best.pt\n",
      "[save] VAEWithSurrogateBundle → /kaggle/working/vae_sur_leaky_last.pt\n",
      "[SAVE] last → /kaggle/working/vae_sur_leaky_last.pt\n",
      "{'best_total': 0.12818251283424895}\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# VAE + Fresh Small Surrogate (Identity adapter) — LEAKY TF MODE\n",
    "#  - CE: teacher forcing with memory='encoder'  (정보 누설, CE≈0)\n",
    "#  - Align: surrogate memory vs encoder memory (MSEᵐ + COS)\n",
    "# ============================================================\n",
    "\n",
    "import os, math, time, json, random, datetime\n",
    "from typing import List, Tuple\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from tqdm import tqdm\n",
    "import esm\n",
    "\n",
    "# -------------------\n",
    "# Config\n",
    "# -------------------\n",
    "SEED   = 42\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.manual_seed(SEED); random.seed(SEED)\n",
    "if DEVICE.type == \"cuda\": torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "MAX_LEN     = 512\n",
    "BATCH_SIZE  = 128\n",
    "TRAIN_RATIO = 0.9\n",
    "\n",
    "EMB_DIM     = 256\n",
    "LATENT_DIM  = 256\n",
    "NUM_LAYERS  = 4      # encoder/decoder layers (set to match your ckpt)\n",
    "NUM_HEADS   = 4\n",
    "FFN_DIM     = 512\n",
    "DROPOUT     = 0.10\n",
    "\n",
    "# Loss weights\n",
    "W_CE  = 1.0\n",
    "W_MSE = 5.0\n",
    "W_COS = 5.0\n",
    "\n",
    "# -------------------\n",
    "# Data (EOS-less TF)\n",
    "# -------------------\n",
    "class ProteinDataset(Dataset):\n",
    "    def __init__(self, sequences: List[str], alphabet, max_len: int = MAX_LEN):\n",
    "        self.alphabet = alphabet\n",
    "        self.max_len  = max_len\n",
    "        self.tokens, self.lengths = [], []\n",
    "        for s in sequences:\n",
    "            ids = [alphabet.get_idx(c) for c in s][:max_len]\n",
    "            if len(ids) == 0: continue\n",
    "            self.tokens.append(torch.tensor(ids, dtype=torch.long))\n",
    "            self.lengths.append(len(ids))\n",
    "    def __len__(self): return len(self.tokens)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.tokens[idx], self.lengths[idx]\n",
    "\n",
    "def _collate_eosless(batch, pad_idx: int):\n",
    "    seqs, lens = zip(*batch)\n",
    "    x = pad_sequence(seqs, batch_first=True, padding_value=pad_idx)\n",
    "    Lmax = x.size(1)\n",
    "    m = torch.zeros((len(seqs), Lmax), dtype=torch.bool)\n",
    "    for i, L in enumerate(lens): m[i, :L] = True\n",
    "    return x, m\n",
    "\n",
    "def make_loaders(sequences, alphabet, batch_size=BATCH_SIZE, train_ratio=TRAIN_RATIO, max_len=MAX_LEN, seed=SEED):\n",
    "    PAD = getattr(alphabet, \"pad_idx\", alphabet.get_idx(\"<pad>\"))\n",
    "    ds = ProteinDataset(sequences, alphabet, max_len=max_len)\n",
    "    n_tr = int(len(ds)*train_ratio); n_va = len(ds)-n_tr\n",
    "    g = torch.Generator().manual_seed(seed)\n",
    "    ds_tr, ds_va = random_split(ds, [n_tr, n_va], generator=g)\n",
    "    dl_tr = DataLoader(ds_tr, batch_size=batch_size, shuffle=True,  num_workers=2,\n",
    "                       pin_memory=(DEVICE.type==\"cuda\"),\n",
    "                       collate_fn=lambda b: _collate_eosless(b, PAD), drop_last=True)\n",
    "    dl_va = DataLoader(ds_va, batch_size=batch_size, shuffle=False, num_workers=2,\n",
    "                       pin_memory=(DEVICE.type==\"cuda\"),\n",
    "                       collate_fn=lambda b: _collate_eosless(b, PAD), drop_last=False)\n",
    "    return dl_tr, dl_va, PAD\n",
    "\n",
    "# -------------------\n",
    "# Models\n",
    "# -------------------\n",
    "class SmallTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size: int, emb_dim: int, layers: int, heads: int,\n",
    "                 ffn_dim: int, max_len: int, pad_idx: int):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(vocab_size, emb_dim, padding_idx=pad_idx)\n",
    "        self.pos = nn.Parameter(torch.zeros(1, max_len, emb_dim))\n",
    "        enc_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=emb_dim, nhead=heads, dim_feedforward=ffn_dim,\n",
    "            batch_first=True, activation=\"gelu\", dropout=DROPOUT\n",
    "        )\n",
    "        self.enc = nn.TransformerEncoder(enc_layer, layers)\n",
    "        self.ln  = nn.LayerNorm(emb_dim)\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        mask = x != self.emb.padding_idx\n",
    "        h = self.emb(x) + self.pos[:, :x.size(1), :]\n",
    "        h = self.enc(h, src_key_padding_mask=~mask)\n",
    "        return self.ln(h), mask\n",
    "\n",
    "class VAETransformerDecoder(nn.Module):\n",
    "    def __init__(self, encoder: SmallTransformer, vocab_size: int,\n",
    "                 latent_dim: int, emb_dim: int,\n",
    "                 num_layers: int, num_heads: int, ffn_dim: int,\n",
    "                 max_len: int, pad_token: int, bos_token: int):\n",
    "        super().__init__()\n",
    "        self.encoder   = encoder\n",
    "        self.pad_token = pad_token\n",
    "        self.bos_token = bos_token\n",
    "        self.to_mu      = nn.Linear(emb_dim, latent_dim)\n",
    "        self.to_logvar  = nn.Linear(emb_dim, latent_dim)\n",
    "        self.latent2emb = nn.Linear(latent_dim, emb_dim)\n",
    "        self.dec_emb = nn.Embedding(vocab_size, emb_dim, padding_idx=pad_token)\n",
    "        self.dec_pos = nn.Parameter(torch.zeros(1, max_len, emb_dim))\n",
    "        dec_layer = nn.TransformerDecoderLayer(\n",
    "            d_model=emb_dim, nhead=num_heads, dim_feedforward=ffn_dim,\n",
    "            dropout=DROPOUT, batch_first=True\n",
    "        )\n",
    "        self.decoder = nn.TransformerDecoder(dec_layer, num_layers)\n",
    "        self.out     = nn.Linear(emb_dim, vocab_size)\n",
    "\n",
    "class Z2MemorySurrogate(nn.Module):\n",
    "    \"\"\"Small surrogate: d_model=256, layers=2, heads=4.\"\"\"\n",
    "    def __init__(self, d_model: int, latent_dim: int, max_len: int,\n",
    "                 layers: int = 2, heads: int = 4, ffn_dim: int = None, dropout: float = DROPOUT):\n",
    "        super().__init__()\n",
    "        if ffn_dim is None: ffn_dim = 3 * d_model\n",
    "        self.pos   = nn.Parameter(torch.zeros(1, max_len, d_model))\n",
    "        self.token = nn.Parameter(torch.zeros(1, 1, d_model))\n",
    "        self.z_proj= nn.Linear(latent_dim, d_model)\n",
    "        self.z_ln  = nn.LayerNorm(d_model)\n",
    "        enc_layer  = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, nhead=heads, dim_feedforward=ffn_dim,\n",
    "            batch_first=True, activation=\"gelu\", dropout=dropout\n",
    "        )\n",
    "        self.enc   = nn.TransformerEncoder(enc_layer, num_layers=layers)\n",
    "        self.out_ln= nn.LayerNorm(d_model)\n",
    "    def forward(self, z, mask_bool, causal_self: bool = False):\n",
    "        B, L = mask_bool.shape\n",
    "        base = self.token.expand(B, L, -1) + self.pos[:, :L, :]\n",
    "        zemb = self.z_ln(self.z_proj(z)).unsqueeze(1).expand(-1, L, -1)\n",
    "        h = base + zemb\n",
    "        src_mask = None\n",
    "        if causal_self:\n",
    "            src_mask = torch.triu(torch.full((L, L), float('-inf'), device=h.device), diagonal=1)\n",
    "        h = self.enc(h, mask=src_mask, src_key_padding_mask=~mask_bool)\n",
    "        return self.out_ln(h), mask_bool\n",
    "\n",
    "class VAEWithSurrogateBundle(nn.Module):\n",
    "    def __init__(self, vae, surrogate, sur_adapter):\n",
    "        super().__init__()\n",
    "        self.vae, self.surrogate, self.sur_adapter = vae, surrogate, sur_adapter\n",
    "\n",
    "    def _teacher_logits(self, x: torch.Tensor, x_mask: torch.Tensor,\n",
    "                        memory: torch.Tensor, memory_mask: torch.Tensor,\n",
    "                        z: torch.Tensor, inject_z: bool = True):\n",
    "        B, L = x.size()\n",
    "        dec_in = torch.full((B, L), self.vae.bos_token, device=x.device, dtype=torch.long)\n",
    "        dec_in[:, 1:] = x[:, :-1]\n",
    "        tgt = self.vae.dec_emb(dec_in) + self.vae.dec_pos[:, :L, :]\n",
    "        if inject_z:\n",
    "            z_emb = self.vae.latent2emb(z).unsqueeze(1).expand(-1, L, -1)\n",
    "            tgt = tgt + z_emb\n",
    "        tgt_mask = nn.Transformer.generate_square_subsequent_mask(L).to(x.device)\n",
    "        h_dec = self.vae.decoder(\n",
    "            tgt=tgt, memory=memory,\n",
    "            tgt_mask=tgt_mask,\n",
    "            tgt_key_padding_mask=~x_mask,\n",
    "            memory_key_padding_mask=(~memory_mask) if (memory_mask is not None) else None\n",
    "        )\n",
    "        return self.vae.out(h_dec)\n",
    "\n",
    "    def forward(self, x: torch.Tensor,\n",
    "                use_surrogate: bool = False,\n",
    "                deterministic_z: bool = False,\n",
    "                inject_z: bool = True):\n",
    "        x_mask = (x != self.vae.pad_token)\n",
    "        h_enc, enc_mask = self.vae.encoder(x)\n",
    "        pooled = (h_enc * enc_mask.unsqueeze(-1)).sum(1) / enc_mask.sum(1, True).clamp_min(1)\n",
    "        mu, logvar = self.vae.to_mu(pooled), self.vae.to_logvar(pooled)\n",
    "        z = mu if deterministic_z else mu + torch.randn_like(mu) * torch.exp(0.5 * logvar)\n",
    "        if not use_surrogate:\n",
    "            logits = self._teacher_logits(x, x_mask, h_enc, enc_mask, z, inject_z=inject_z)\n",
    "            return logits, mu, logvar, (h_enc, enc_mask, z, None, None)\n",
    "        sur_mem, sur_mask = self.surrogate(z, enc_mask, causal_self=False)\n",
    "        mem = sur_mem if (self.sur_adapter is None) else self.sur_adapter(sur_mem)\n",
    "        logits = self._teacher_logits(x, x_mask, mem, sur_mask, z, inject_z=inject_z)\n",
    "        return logits, mu, logvar, (h_enc, enc_mask, z, sur_mem, sur_mask)\n",
    "\n",
    "    def save(self, path: str, extra_meta: dict = None):\n",
    "        payload = {\n",
    "            \"bundle_version\": 2,\n",
    "            \"vae\":         self.vae.state_dict(),\n",
    "            \"surrogate\":   self.surrogate.state_dict(),\n",
    "            \"sur_adapter\": None,  # Identity\n",
    "            \"meta\": {\"saved_at\": datetime.datetime.now().isoformat()}\n",
    "        }\n",
    "        if extra_meta: payload[\"meta\"].update(extra_meta)\n",
    "        torch.save(payload, path)\n",
    "        print(f\"[save] VAEWithSurrogateBundle → {path}\")\n",
    "\n",
    "# -------------------\n",
    "# Loss helpers\n",
    "# -------------------\n",
    "def masked_ce(logits: torch.Tensor, tgt: torch.Tensor, mask: torch.Tensor, ignore_index: int, label_smoothing: float = 0.0):\n",
    "    B,L,V = logits.shape\n",
    "    flat_logits = logits.view(B*L, V)\n",
    "    flat_tgt    = tgt.view(B*L)\n",
    "    valid       = mask.view(B*L)\n",
    "    if valid.sum() == 0: return flat_logits.new_zeros(())\n",
    "    return F.cross_entropy(flat_logits[valid], flat_tgt[valid],\n",
    "                           ignore_index=ignore_index, label_smoothing=label_smoothing)\n",
    "\n",
    "def masked_mse_mean(a: torch.Tensor, b: torch.Tensor, mask: torch.Tensor):\n",
    "    diff2 = (a - b).pow(2).mean(-1)   # mean over channels\n",
    "    denom = mask.float().sum().clamp_min(1)\n",
    "    return (diff2 * mask.float()).sum() / denom\n",
    "\n",
    "def masked_cosine_loss(a: torch.Tensor, b: torch.Tensor, mask: torch.Tensor):\n",
    "    cs = F.cosine_similarity(a, b, dim=-1, eps=1e-8)\n",
    "    loss = 1.0 - cs\n",
    "    denom = mask.float().sum().clamp_min(1)\n",
    "    return (loss * mask.float()).sum() / denom\n",
    "\n",
    "# -------------------\n",
    "# Build bundle (load VAE ckpt['model_sd']; surrogate fresh; adapter Identity)\n",
    "# -------------------\n",
    "def build_bundle_from_vae_ckpt(vae_ckpt_path: str, alphabet) -> VAEWithSurrogateBundle:\n",
    "    PAD = getattr(alphabet, \"pad_idx\", alphabet.get_idx(\"<pad>\"))\n",
    "    BOS = getattr(alphabet, \"bos_idx\", alphabet.get_idx(\"<cls>\"))\n",
    "    vocab = len(alphabet.all_toks)\n",
    "\n",
    "    enc = SmallTransformer(vocab_size=vocab, emb_dim=EMB_DIM,\n",
    "                           layers=NUM_LAYERS, heads=NUM_HEADS, ffn_dim=FFN_DIM,\n",
    "                           max_len=MAX_LEN, pad_idx=PAD).to(DEVICE)\n",
    "    vae = VAETransformerDecoder(encoder=enc, vocab_size=vocab,\n",
    "                                latent_dim=LATENT_DIM, emb_dim=EMB_DIM,\n",
    "                                num_layers=NUM_LAYERS, num_heads=NUM_HEADS, ffn_dim=FFN_DIM,\n",
    "                                max_len=MAX_LEN, pad_token=PAD, bos_token=BOS).to(DEVICE)\n",
    "\n",
    "    ckpt = torch.load(vae_ckpt_path, map_location=\"cpu\")\n",
    "    assert \"model_sd\" in ckpt, \"VAE ckpt must contain 'model_sd'.\"\n",
    "    miss = vae.load_state_dict(ckpt[\"model_sd\"], strict=False)\n",
    "    print(f\"[VAE] loaded from 'model_sd'  missing={len(miss.missing_keys)}  unexpected={len(miss.unexpected_keys)}\")\n",
    "\n",
    "    sur = Z2MemorySurrogate(d_model=256, latent_dim=LATENT_DIM, max_len=MAX_LEN,\n",
    "                            layers=2, heads=4, ffn_dim=3*256, dropout=DROPOUT).to(DEVICE)\n",
    "    adapter = nn.Identity().to(DEVICE)  # ← 완전 제거(Identity)\n",
    "    print(\"[SUR] initialized small surrogate (2L,4H,256D)\")\n",
    "    print(\"[ADAPTER] Identity (no params)\")\n",
    "\n",
    "    bundle = VAEWithSurrogateBundle(vae, sur, adapter).to(DEVICE)\n",
    "    return bundle\n",
    "\n",
    "# -------------------\n",
    "# Teacher-forced logits with selectable memory (LEAKY TF)\n",
    "# -------------------\n",
    "def tf_logits_with_memory(bundle, x, xmask, z, mem, mem_mask, inject_z=True):\n",
    "    return bundle._teacher_logits(x, xmask, mem, mem_mask, z, inject_z=inject_z)\n",
    "\n",
    "# -------------------\n",
    "# Training (LEAKY TF): CE uses encoder memory; Align mem_sur ↔ h_enc\n",
    "# -------------------\n",
    "def finetune_surrogate_adapter_leaky(\n",
    "    bundle: VAEWithSurrogateBundle,\n",
    "    sequences,\n",
    "    alphabet,\n",
    "    out_best=\"./vae_sur_leaky_best.pt\",\n",
    "    out_last=\"./vae_sur_leaky_last.pt\",\n",
    "    epochs=3,\n",
    "    batch_size=128,\n",
    "    lr=1e-4,\n",
    "    weight_decay=0.01,\n",
    "    label_smoothing=0.0,\n",
    "    grad_clip=1.0,\n",
    "    log_every=20,\n",
    "):\n",
    "    dl_tr, dl_va, PAD = make_loaders(sequences, alphabet, batch_size=batch_size)\n",
    "\n",
    "    # Freeze VAE, train surrogate (+ adapter if any; Identity → 0 params)\n",
    "    for p in bundle.vae.parameters(): p.requires_grad = False\n",
    "    for p in bundle.surrogate.parameters(): p.requires_grad = True\n",
    "    adp_params = list(getattr(bundle.sur_adapter, \"parameters\", lambda: [])())\n",
    "    for p in adp_params: p.requires_grad = True\n",
    "\n",
    "    groups = [{\"params\":[p for p in bundle.surrogate.parameters() if p.requires_grad]}]\n",
    "    if any(p.requires_grad for p in adp_params):\n",
    "        groups.append({\"params\":[p for p in adp_params if p.requires_grad]})\n",
    "\n",
    "    opt = torch.optim.AdamW(groups, lr=lr, weight_decay=weight_decay, betas=(0.9, 0.95))\n",
    "    scaler = torch.amp.GradScaler('cuda', enabled=(DEVICE.type==\"cuda\"))\n",
    "\n",
    "    print(f\"[info] trainable params = {sum(p.numel() for g in groups for p in g['params']):,}\")\n",
    "    print(f\"[info] frozen    params = {sum(p.numel() for p in bundle.vae.parameters()):,}\")\n",
    "\n",
    "    best = float(\"inf\")\n",
    "\n",
    "    for ep in range(1, epochs+1):\n",
    "        bundle.train()\n",
    "        pbar = tqdm(dl_tr, desc=f\"[Train(leaky enc-mem) ep={ep}/{epochs}]\", dynamic_ncols=True)\n",
    "        run = defaultdict(float)\n",
    "\n",
    "        for i, (x, xmask) in enumerate(pbar, start=1):\n",
    "            x, xmask = x.to(DEVICE), xmask.to(DEVICE)\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "\n",
    "            # Encode → z\n",
    "            h_enc, enc_mask = bundle.vae.encoder(x)\n",
    "            pooled = (h_enc * enc_mask.unsqueeze(-1)).sum(1) / enc_mask.sum(1, True).clamp_min(1)\n",
    "            mu, logvar = bundle.vae.to_mu(pooled), bundle.vae.to_logvar(pooled)\n",
    "            z = mu + torch.randn_like(mu) * torch.exp(0.5 * logvar)\n",
    "\n",
    "            # Surrogate mem (to be aligned)\n",
    "            sur_mem, sur_mask = bundle.surrogate(z, enc_mask, causal_self=False)\n",
    "            mem_sur = sur_mem if (bundle.sur_adapter is None) else bundle.sur_adapter(sur_mem)\n",
    "\n",
    "            with torch.amp.autocast('cuda', enabled=(DEVICE.type==\"cuda\")):\n",
    "                # ★ CE with LEAKY memory = encoder h_enc\n",
    "                logits = tf_logits_with_memory(bundle, x, xmask, z, h_enc, enc_mask, inject_z=True)\n",
    "                vm_ce  = (xmask & enc_mask)\n",
    "                ce = masked_ce(logits, x, vm_ce, ignore_index=PAD, label_smoothing=label_smoothing)\n",
    "\n",
    "                # Align surrogate memory ↔ encoder memory\n",
    "                vm_align = (xmask & enc_mask & sur_mask)\n",
    "                mse = masked_mse_mean(mem_sur, h_enc, vm_align)\n",
    "                cos = masked_cosine_loss(mem_sur, h_enc, vm_align)\n",
    "\n",
    "                loss = W_CE*ce + W_MSE*mse + W_COS*cos\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.unscale_(opt)\n",
    "            if grad_clip and grad_clip>0:\n",
    "                torch.nn.utils.clip_grad_norm_([p for g in groups for p in g[\"params\"]], grad_clip)\n",
    "            scaler.step(opt); scaler.update()\n",
    "\n",
    "            # logs\n",
    "            run[\"loss\"] += float(loss); run[\"ce\"] += float(ce); run[\"mse\"] += float(mse); run[\"cos\"] += float(cos); run[\"n\"] += 1\n",
    "            if i % log_every == 0 or i == 1:\n",
    "                pbar.set_postfix({\"loss\": f\"{float(loss):.3f}\", \"ce\": f\"{float(ce):.4f}\", \"mse\": f\"{float(mse):.3f}\", \"cos\": f\"{float(cos):.3f}\"})\n",
    "\n",
    "        # ---- epoch summary\n",
    "        n = max(1, int(run[\"n\"]))\n",
    "        tr = {k: run[k]/n for k in [\"loss\",\"ce\",\"mse\",\"cos\"]}\n",
    "\n",
    "        # ---- validation (deterministic z=μ)\n",
    "        bundle.eval()\n",
    "        val = defaultdict(float)\n",
    "        with torch.no_grad():\n",
    "            for x, xmask in tqdm(dl_va, desc=\"[Val(leaky)]\", dynamic_ncols=True):\n",
    "                x, xmask = x.to(DEVICE), xmask.to(DEVICE)\n",
    "                h_enc, enc_mask = bundle.vae.encoder(x)\n",
    "                pooled = (h_enc * enc_mask.unsqueeze(-1)).sum(1) / enc_mask.sum(1, True).clamp_min(1)\n",
    "                mu, logvar = bundle.vae.to_mu(pooled), bundle.vae.to_logvar(pooled)\n",
    "                z = mu  # deterministic for eval\n",
    "\n",
    "                sur_mem, sur_mask = bundle.surrogate(z, enc_mask, causal_self=False)\n",
    "                mem_sur = sur_mem if (bundle.sur_adapter is None) else bundle.sur_adapter(sur_mem)\n",
    "\n",
    "                logits = tf_logits_with_memory(bundle, x, xmask, z, h_enc, enc_mask, inject_z=True)\n",
    "                vm_ce  = (xmask & enc_mask)\n",
    "                ce = masked_ce(logits, x, vm_ce, ignore_index=PAD, label_smoothing=0.0)\n",
    "                vm_align = (xmask & enc_mask & sur_mask)\n",
    "                mse = masked_mse_mean(mem_sur, h_enc, vm_align)\n",
    "                cos = masked_cosine_loss(mem_sur, h_enc, vm_align)\n",
    "\n",
    "                total = W_CE*ce + W_MSE*mse + W_COS*cos\n",
    "                val[\"loss\"] += float(total); val[\"ce\"] += float(ce); val[\"mse\"] += float(mse); val[\"cos\"] += float(cos); val[\"n\"] += 1\n",
    "\n",
    "        n = max(1, int(val[\"n\"]))\n",
    "        va = {k: val[k]/n for k in [\"loss\",\"ce\",\"mse\",\"cos\"]}\n",
    "\n",
    "        print(f\"[Ep {ep}] train: loss={tr['loss']:.3f} ce={tr['ce']:.4f} mse={tr['mse']:.3f} cos={tr['cos']:.3f} | \"\n",
    "              f\"valid: loss={va['loss']:.3f} ce={va['ce']:.4f} mse={va['mse']:.3f} cos={va['cos']:.3f}\")\n",
    "\n",
    "        if va[\"loss\"] < best:\n",
    "            best = va[\"loss\"]\n",
    "            bundle.save(out_best, extra_meta={\"best_val_total\": best, \"leaky_tf\": \"encoder\"})\n",
    "            print(f\"[SAVE] best → {out_best}\")\n",
    "\n",
    "    bundle.save(out_last, extra_meta={\"best_val_total\": best, \"leaky_tf\": \"encoder\"})\n",
    "    print(f\"[SAVE] last → {out_last}\")\n",
    "    return {\"best_total\": best}\n",
    "\n",
    "# -------------------\n",
    "# Smoke check (eval + deterministic μ): ENC vs SUR TF metrics\n",
    "# -------------------\n",
    "@torch.no_grad()\n",
    "def smoke_check(bundle, sequences, alphabet, batch_size=8, nbatches=2):\n",
    "    dl, _, PAD = make_loaders(sequences, alphabet, batch_size=batch_size, train_ratio=1.0)\n",
    "\n",
    "    def masked_acc(logits, tgt, mask):\n",
    "        if mask.sum() == 0: return 0.0\n",
    "        pred = logits.argmax(dim=-1)\n",
    "        correct = ((pred == tgt) & mask).float().sum()\n",
    "        total   = mask.float().sum().clamp_min(1)\n",
    "        return float(correct / total)\n",
    "\n",
    "    was_training = bundle.training\n",
    "    bundle.eval()\n",
    "\n",
    "    enc_ce = enc_acc = 0.0\n",
    "    sur_ce = sur_acc = 0.0\n",
    "    first_done = False\n",
    "\n",
    "    it = iter(dl)\n",
    "    for _ in tqdm(range(nbatches), desc=\"[Smoke|eval] ENC vs SUR (TF)\", dynamic_ncols=True):\n",
    "        try: x, mlen = next(it)\n",
    "        except StopIteration: it = iter(dl); x, mlen = next(it)\n",
    "        x, mlen = x.to(DEVICE), mlen.to(DEVICE)\n",
    "\n",
    "        # deterministic z\n",
    "        h_enc, enc_mask = bundle.vae.encoder(x)\n",
    "        pooled = (h_enc * enc_mask.unsqueeze(-1)).sum(1) / enc_mask.sum(1, True).clamp_min(1)\n",
    "        mu = bundle.vae.to_mu(pooled); z = mu\n",
    "\n",
    "        # ENC TF logits (leaky)\n",
    "        logits_e = bundle._teacher_logits(x, mlen, h_enc, enc_mask, z, inject_z=True)\n",
    "        vm_e = (mlen & enc_mask)\n",
    "        ce_e = masked_ce(logits_e, x, vm_e, ignore_index=PAD); acc_e = masked_acc(logits_e, x, vm_e)\n",
    "\n",
    "        # SUR TF logits\n",
    "        sur_mem, sur_mask = bundle.surrogate(z, enc_mask, causal_self=False)\n",
    "        mem_s = sur_mem if (bundle.sur_adapter is None) else bundle.sur_adapter(sur_mem)\n",
    "        logits_s = bundle._teacher_logits(x, mlen, mem_s, sur_mask, z, inject_z=True)\n",
    "        vm_s = (mlen & sur_mask)\n",
    "        ce_s = masked_ce(logits_s, x, vm_s, ignore_index=PAD); acc_s = masked_acc(logits_s, x, vm_s)\n",
    "\n",
    "        enc_ce += float(ce_e); enc_acc += float(acc_e)\n",
    "        sur_ce += float(ce_s); sur_acc += float(acc_s)\n",
    "\n",
    "        if not first_done:\n",
    "            B, L = x.shape\n",
    "            print(f\"[shapes] x={(B,L)}  h_enc={tuple(h_enc.shape)}  sur_mem={tuple(sur_mem.shape)}\")\n",
    "            first_done = True\n",
    "\n",
    "    enc_ce /= max(1, nbatches); sur_ce /= max(1, nbatches)\n",
    "    enc_acc/= max(1, nbatches); sur_acc/= max(1, nbatches)\n",
    "    print(f\"[ENC-TF]  CE={enc_ce:.4f}  PPL={math.exp(enc_ce):.2f}  ACC@1={enc_acc*100:.2f}%\")\n",
    "    print(f\"[SUR-TF]  CE={sur_ce:.4f}  PPL={math.exp(sur_ce):.2f}  ACC@1={sur_acc*100:.2f}%\")\n",
    "    print(f\"[Δ SUR-ENC] ΔCE={sur_ce-enc_ce:+.4f}  ΔACC={(sur_acc-enc_acc)*100:+.2f}%\")\n",
    "\n",
    "    if was_training: bundle.train()\n",
    "\n",
    "# ============================================================\n",
    "# Usage\n",
    "# ============================================================\n",
    "# 0) alphabet & sequences 준비\n",
    "_, alphabet = esm.pretrained.esm2_t33_650M_UR50D()\n",
    "# sequences = [...]  # List[str]  ← 사용자 데이터\n",
    "assert 'sequences' in globals(), \"Provide your `sequences: List[str]`.\"\n",
    "\n",
    "# 1) VAE ckpt 로딩 + surrogate fresh (adapter=Identity)\n",
    "VAE_CKPT = \"/kaggle/input/esms-vae/pytorch/default2/1/vae_epoch380.pt\"\n",
    "bundle = build_bundle_from_vae_ckpt(VAE_CKPT, alphabet)\n",
    "bundle.save(\"/kaggle/working/vae_sur_leaky_init.pt\")\n",
    "\n",
    "# 2) 스모크 체크 (eval + μ)\n",
    "smoke_check(bundle, sequences, alphabet, batch_size=8, nbatches=2)\n",
    "\n",
    "# 3) 파인튜닝 (LEAKY: CE with encoder memory)\n",
    "stats = finetune_surrogate_adapter_leaky(\n",
    "    bundle, sequences, alphabet,\n",
    "    out_best=\"/kaggle/working/vae_sur_leaky_best.pt\",\n",
    "    out_last=\"/kaggle/working/vae_sur_leaky_last.pt\",\n",
    "    epochs=4, batch_size=128, lr=1e-4, log_every=20\n",
    ")\n",
    "print(stats)\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7398719,
     "sourceId": 11784200,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 351326,
     "modelInstanceId": 330981,
     "sourceId": 404999,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 441972,
     "modelInstanceId": 424482,
     "sourceId": 559836,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 17279.604902,
   "end_time": "2025-09-05T18:30:55.140011",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-09-05T13:42:55.535109",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
