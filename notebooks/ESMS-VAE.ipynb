{
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.11",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "sourceId": 11784200,
     "sourceType": "datasetVersion",
     "datasetId": 7398719
    },
    {
     "sourceId": 404999,
     "sourceType": "modelInstanceVersion",
     "isSourceIdPinned": true,
     "modelInstanceId": 330981,
     "modelId": 351326
    }
   ],
   "dockerImageVersionId": 31040,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": false
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": "## 1. Install Dependencies and device setup",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "!pip install torch\n!pip install fair-esm\n!pip install biopython",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-05-22T03:09:28.577760Z",
     "iopub.execute_input": "2025-05-22T03:09:28.578361Z",
     "iopub.status.idle": "2025-05-22T03:10:53.616416Z",
     "shell.execute_reply.started": "2025-05-22T03:09:28.578313Z",
     "shell.execute_reply": "2025-05-22T03:10:53.615697Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\nDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m363.4/363.4 MB\u001B[0m \u001B[31m4.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m0:00:01\u001B[0m00:01\u001B[0m\n\u001B[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m664.8/664.8 MB\u001B[0m \u001B[31m2.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m0:00:01\u001B[0m00:01\u001B[0m\n\u001B[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m211.5/211.5 MB\u001B[0m \u001B[31m3.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m0:00:01\u001B[0m00:01\u001B[0m\n\u001B[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m56.3/56.3 MB\u001B[0m \u001B[31m3.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m0:00:01\u001B[0m00:01\u001B[0m\n\u001B[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m127.9/127.9 MB\u001B[0m \u001B[31m13.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m:00:01\u001B[0m00:01\u001B[0m\n\u001B[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m207.5/207.5 MB\u001B[0m \u001B[31m8.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m0:00:01\u001B[0m00:01\u001B[0m\n\u001B[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m21.1/21.1 MB\u001B[0m \u001B[31m78.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m:00:01\u001B[0m00:01\u001B[0m\n\u001B[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.9.41\n    Uninstalling nvidia-nvjitlink-cu12-12.9.41:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.9.41\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.10.19\n    Uninstalling nvidia-curand-cu12-10.3.10.19:\n      Successfully uninstalled nvidia-curand-cu12-10.3.10.19\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.4.0.6\n    Uninstalling nvidia-cufft-cu12-11.4.0.6:\n      Successfully uninstalled nvidia-cufft-cu12-11.4.0.6\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.9.0.13\n    Uninstalling nvidia-cublas-cu12-12.9.0.13:\n      Successfully uninstalled nvidia-cublas-cu12-12.9.0.13\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.9.5\n    Uninstalling nvidia-cusparse-cu12-12.5.9.5:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.9.5\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.7.4.40\n    Uninstalling nvidia-cusolver-cu12-11.7.4.40:\n      Successfully uninstalled nvidia-cusolver-cu12-11.7.4.40\nSuccessfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\nCollecting fair-esm\n  Downloading fair_esm-2.0.0-py3-none-any.whl.metadata (37 kB)\nDownloading fair_esm-2.0.0-py3-none-any.whl (93 kB)\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m93.1/93.1 kB\u001B[0m \u001B[31m1.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m0:00:01\u001B[0m\n\u001B[?25hInstalling collected packages: fair-esm\nSuccessfully installed fair-esm-2.0.0\nCollecting biopython\n  Downloading biopython-1.85-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from biopython) (1.26.4)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->biopython) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->biopython) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->biopython) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->biopython) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->biopython) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->biopython) (2.4.1)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->biopython) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->biopython) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->biopython) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->biopython) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->biopython) (2024.2.0)\nDownloading biopython-1.85-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m3.3/3.3 MB\u001B[0m \u001B[31m33.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\n\u001B[?25hInstalling collected packages: biopython\nSuccessfully installed biopython-1.85\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "source": "import os\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader, random_split\nfrom torch.cuda.amp import autocast, GradScaler\nfrom torch.nn.functional import cross_entropy, mse_loss, cosine_similarity\nimport esm\nfrom tqdm import tqdm\nfrom torch.nn.utils.rnn import pad_sequence",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-05-22T03:11:03.101442Z",
     "iopub.execute_input": "2025-05-22T03:11:03.102173Z",
     "iopub.status.idle": "2025-05-22T03:11:06.736085Z",
     "shell.execute_reply.started": "2025-05-22T03:11:03.102144Z",
     "shell.execute_reply": "2025-05-22T03:11:06.735573Z"
    }
   },
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "source": "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-05-22T03:11:08.470023Z",
     "iopub.execute_input": "2025-05-22T03:11:08.470409Z",
     "iopub.status.idle": "2025-05-22T03:11:08.556687Z",
     "shell.execute_reply.started": "2025-05-22T03:11:08.470386Z",
     "shell.execute_reply": "2025-05-22T03:11:08.555960Z"
    }
   },
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "source": "## 2. Data Loading",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "from Bio import SeqIO\nseq_path='/kaggle/input/uniref50-sub/uniref50_subsample.fasta'\nsequences=[]\nfor seq_record in SeqIO.parse(seq_path, \"fasta\"):\n    sequences.append(str(seq_record.seq))\nprint(len(sequences))",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-05-22T03:11:10.763084Z",
     "iopub.execute_input": "2025-05-22T03:11:10.763416Z",
     "iopub.status.idle": "2025-05-22T03:11:17.756937Z",
     "shell.execute_reply.started": "2025-05-22T03:11:10.763392Z",
     "shell.execute_reply": "2025-05-22T03:11:17.756329Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "1000000\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "source": [
    "import random\n",
    "\n",
    "# 예시 리스트\n",
    "items = sequences\n",
    "\n",
    "# 1) 중복 없이 k개 샘플링\n",
    "k = 14000\n",
    "sequences= random.sample(items, k)"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-05-21T04:40:48.381259Z",
     "iopub.execute_input": "2025-05-21T04:40:48.382017Z",
     "iopub.status.idle": "2025-05-21T04:40:48.395828Z",
     "shell.execute_reply.started": "2025-05-21T04:40:48.381990Z",
     "shell.execute_reply": "2025-05-21T04:40:48.395195Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## 3. Model & Dataset Definitions",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "from torch.utils.data import Dataset, DataLoader, random_split\nimport torch.nn as nn\n\n# ── Dataset & Collate ──\nclass ProteinDataset(Dataset):\n    def __init__(self, sequences, alphabet):\n        self.sequences = [seq[:MAX_LEN] for seq in sequences]\n        self.alphabet  = alphabet\n\n    def __len__(self):\n        return len(self.sequences)\n\n    def __getitem__(self, idx):\n        idxs = [self.alphabet.get_idx(c) for c in self.sequences[idx]]\n        return torch.tensor(idxs, dtype=torch.long)\n\ndef collate_fn(batch, pad_idx):\n    padded = pad_sequence(batch, batch_first=True, padding_value=pad_idx)\n    mask   = (padded != pad_idx)\n    return padded, mask\n\n# ── Teacher & Encoder ──\nclass SmallTransformer(nn.Module):\n    def __init__(self, vocab_size, emb_dim, layers, heads, ffn_dim, max_len, pad_idx):\n        super().__init__()\n        self.emb = nn.Embedding(vocab_size, emb_dim, padding_idx=pad_idx)\n        self.pos = nn.Parameter(torch.zeros(1, max_len, emb_dim))\n        layer   = nn.TransformerEncoderLayer(\n            d_model=emb_dim, nhead=heads,\n            dim_feedforward=ffn_dim, batch_first=True,\n            activation='gelu', dropout=DROPOUT\n        )\n        self.enc = nn.TransformerEncoder(layer, layers)\n        self.ln  = nn.LayerNorm(emb_dim)\n\n    def forward(self, x):\n        mask = x != self.emb.padding_idx\n        h    = self.emb(x) + self.pos[:, :x.size(1), :]\n        h    = self.enc(h, src_key_padding_mask=~mask)\n        return self.ln(h), mask\n\nclass BigTransformer(SmallTransformer):\n    pass  # identical API\n\n# ── Single‐stage VAE ──\nclass VAETransformerDecoder(nn.Module):\n    def __init__(self, encoder, vocab_size, latent_dim=LATENT_DIM,\n                 emb_dim=EMB_DIM, num_layers=NUM_LAYERS, num_heads=NUM_HEADS,\n                 ffn_dim=FFN_DIM, max_len=MAX_LEN, pad_token=0, bos_token=1):\n        super().__init__()\n        self.encoder   = encoder\n        self.pad_token = pad_token\n        self.bos_token = bos_token\n\n        # latent heads\n        self.to_mu     = nn.Linear(emb_dim, latent_dim)\n        self.to_logvar = nn.Linear(emb_dim, latent_dim)\n        self.latent2emb= nn.Linear(latent_dim, emb_dim)\n\n        # decoder\n        self.dec_emb = nn.Embedding(vocab_size, emb_dim, padding_idx=pad_token)\n        self.dec_pos = nn.Parameter(torch.zeros(1, max_len, emb_dim))\n        layer = nn.TransformerDecoderLayer(\n            d_model=emb_dim, nhead=num_heads,\n            dim_feedforward=ffn_dim, dropout=DROPOUT,\n            batch_first=True\n        )\n        self.decoder = nn.TransformerDecoder(layer, num_layers)\n        self.out     = nn.Linear(emb_dim, vocab_size)\n\n    def forward(self, x, mask):\n        # encode\n        h_enc, enc_mask = self.encoder(x)\n        pooled = (h_enc * enc_mask.unsqueeze(-1)).sum(1) / enc_mask.sum(1, True)\n        mu, logvar = self.to_mu(pooled), self.to_logvar(pooled)\n        z = mu + torch.randn_like(mu) * torch.exp(0.5 * logvar)\n\n        # prepare decoder input\n        B, L = x.size()\n        dec_in = torch.full((B, L), self.bos_token, device=x.device, dtype=torch.long)\n        dec_in[:,1:] = x[:,:-1]\n        emb = self.dec_emb(dec_in) + self.dec_pos[:, :L, :]\n        z_emb = self.latent2emb(z).unsqueeze(1).expand(-1, L, -1)\n        emb = emb + z_emb\n\n        tgt_mask = nn.Transformer.generate_square_subsequent_mask(L).to(x.device)\n        h_dec = self.decoder(\n            tgt=emb,\n            memory=h_enc,\n            tgt_mask=tgt_mask,\n            tgt_key_padding_mask=~mask,\n            memory_key_padding_mask=~enc_mask\n        )\n        logits = self.out(h_dec)\n        return logits, mu, logvar, h_enc, enc_mask\n",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-05-22T03:11:19.575548Z",
     "iopub.execute_input": "2025-05-22T03:11:19.575823Z",
     "iopub.status.idle": "2025-05-22T03:11:19.588922Z",
     "shell.execute_reply.started": "2025-05-22T03:11:19.575802Z",
     "shell.execute_reply": "2025-05-22T03:11:19.588384Z"
    }
   },
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "source": "## 3.1 Load Teacher and alphabet",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# load alphabet & teacher\nimport esm\n\n_, alphabet = esm.pretrained.esm2_t33_650M_UR50D()\nPAD_IDX     = alphabet.get_idx('<pad>')\nBOS_IDX     = alphabet.get_idx('<cls>')\nckpt        = torch.load('/kaggle/input/esms/transformers/default/1/distilled_embeddings_two_stage.pt', map_location='cpu')\nteacher     = SmallTransformer(\n        len(alphabet.all_toks), EMB_DIM, NUM_LAYERS, NUM_HEADS, FFN_DIM, MAX_LEN, PAD_IDX\n    ).to(device)\nteacher.load_state_dict(ckpt['student_state_dict'], strict=False)\nteacher.eval()\nif torch.cuda.device_count()>1:\n    teacher = nn.DataParallel(teacher)",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-05-22T03:11:24.226141Z",
     "iopub.execute_input": "2025-05-22T03:11:24.226435Z",
     "iopub.status.idle": "2025-05-22T03:11:46.785164Z",
     "shell.execute_reply.started": "2025-05-22T03:11:24.226413Z",
     "shell.execute_reply": "2025-05-22T03:11:46.783919Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "text": "Downloading: \"https://dl.fbaipublicfiles.com/fair-esm/models/esm2_t33_650M_UR50D.pt\" to /root/.cache/torch/hub/checkpoints/esm2_t33_650M_UR50D.pt\nDownloading: \"https://dl.fbaipublicfiles.com/fair-esm/regression/esm2_t33_650M_UR50D-contact-regression.pt\" to /root/.cache/torch/hub/checkpoints/esm2_t33_650M_UR50D-contact-regression.pt\n",
     "output_type": "stream"
    },
    {
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_35/2764847780.py\u001B[0m in \u001B[0;36m<cell line: 0>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0mPAD_IDX\u001B[0m     \u001B[0;34m=\u001B[0m \u001B[0malphabet\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget_idx\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'<pad>'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      6\u001B[0m \u001B[0mBOS_IDX\u001B[0m     \u001B[0;34m=\u001B[0m \u001B[0malphabet\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget_idx\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'<cls>'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 7\u001B[0;31m \u001B[0mckpt\u001B[0m        \u001B[0;34m=\u001B[0m \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mload\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'/kaggle/input/esms/transformers/default/1/distilled_embeddings_two_stage.pt'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmap_location\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m'cpu'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      8\u001B[0m teacher     = SmallTransformer(\n\u001B[1;32m      9\u001B[0m         \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0malphabet\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mall_toks\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mEMB_DIM\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mNUM_LAYERS\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mNUM_HEADS\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mFFN_DIM\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mMAX_LEN\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mPAD_IDX\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001B[0m in \u001B[0;36mload\u001B[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001B[0m\n\u001B[1;32m   1423\u001B[0m         \u001B[0mpickle_load_args\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m\"encoding\"\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m\"utf-8\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1424\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1425\u001B[0;31m     \u001B[0;32mwith\u001B[0m \u001B[0m_open_file_like\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mf\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"rb\"\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mopened_file\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1426\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0m_is_zipfile\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mopened_file\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1427\u001B[0m             \u001B[0;31m# The zipfile reader is going to advance the current file position.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001B[0m in \u001B[0;36m_open_file_like\u001B[0;34m(name_or_buffer, mode)\u001B[0m\n\u001B[1;32m    749\u001B[0m \u001B[0;32mdef\u001B[0m \u001B[0m_open_file_like\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mname_or_buffer\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmode\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    750\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0m_is_path\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mname_or_buffer\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 751\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0m_open_file\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mname_or_buffer\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmode\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    752\u001B[0m     \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    753\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0;34m\"w\"\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mmode\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, name, mode)\u001B[0m\n\u001B[1;32m    730\u001B[0m \u001B[0;32mclass\u001B[0m \u001B[0m_open_file\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0m_opener\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    731\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m__init__\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mname\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmode\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 732\u001B[0;31m         \u001B[0msuper\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__init__\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mopen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mname\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmode\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    733\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    734\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m__exit__\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: '/kaggle/input/esms/transformers/default/1/distilled_embeddings_two_stage.pt'"
     ],
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/kaggle/input/esms/transformers/default/1/distilled_embeddings_two_stage.pt'",
     "output_type": "error"
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "source": "## 3.2 Define model",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# ── Config ──\nMAX_LEN       = 512\nBATCH_SIZE    = 64\nLATENT_DIM    = 256\nEMB_DIM       = 256\nNUM_LAYERS    = 4\nNUM_HEADS     = 4\nFFN_DIM       = 512\nDROPOUT       = 0.3\n\nLR_PHASE1     = 5e-4    # higher LR for CE‐only warmup\nLR_PHASE2     = 1e-4    # later LR\n\nEPOCHS_PHASE1 = 100      # CE‐only for first 20 epochs\nTOTAL_EPOCHS  = 500\n\nCE_WEIGHT1    = 100.0    # CE weight during phase1\nCE_WEIGHT2    = 1.0     # CE weight afterwards\nKL_WEIGHT     = 0.1\nCOS_WEIGHT    = 5.0\nMSE_WEIGHT    = 5.0",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-05-22T03:10:53.617988Z",
     "iopub.execute_input": "2025-05-22T03:10:53.618241Z",
     "iopub.status.idle": "2025-05-22T03:10:53.622972Z",
     "shell.execute_reply.started": "2025-05-22T03:10:53.618218Z",
     "shell.execute_reply": "2025-05-22T03:10:53.622381Z"
    }
   },
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "source": "import torch.nn as nn\nimport torch\nenc = BigTransformer(\n        len(alphabet.all_toks), EMB_DIM, NUM_LAYERS, NUM_HEADS, FFN_DIM, MAX_LEN, PAD_IDX\n    ).to(device)\nvae = VAETransformerDecoder(\n        encoder=enc,\n        vocab_size=len(alphabet.all_toks),\n        latent_dim=LATENT_DIM,\n        emb_dim=EMB_DIM, num_layers=NUM_LAYERS, num_heads=NUM_HEADS,\n        ffn_dim=FFN_DIM, max_len=MAX_LEN,\n        pad_token=PAD_IDX, bos_token=BOS_IDX\n    ).to(device)\nvae = nn.DataParallel(vae)\n\nscaler = GradScaler()",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-05-22T03:12:41.649845Z",
     "iopub.execute_input": "2025-05-22T03:12:41.650477Z",
     "iopub.status.idle": "2025-05-22T03:12:41.906900Z",
     "shell.execute_reply.started": "2025-05-22T03:12:41.650451Z",
     "shell.execute_reply": "2025-05-22T03:12:41.906009Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "text": "/tmp/ipykernel_35/2854719661.py:16: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  scaler = GradScaler()\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "source": "## 4. Data Preparation",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "from torch.utils.data import DataLoader\nds        = ProteinDataset(sequences, alphabet)\nt,v,s     = int(0.8*len(ds)), int(0.1*len(ds)), len(ds) - int(0.9*len(ds))\ntrain_ds, val_ds, test_ds = random_split(ds, [t,v,s])\ntrain_loader = DataLoader(train_ds, BATCH_SIZE, shuffle=True,\n                              collate_fn=lambda b: collate_fn(b, PAD_IDX))\nval_loader   = DataLoader(val_ds,   BATCH_SIZE, shuffle=False,\n                              collate_fn=lambda b: collate_fn(b, PAD_IDX))\n",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-05-22T03:12:48.175000Z",
     "iopub.execute_input": "2025-05-22T03:12:48.175471Z",
     "iopub.status.idle": "2025-05-22T03:12:48.465593Z",
     "shell.execute_reply.started": "2025-05-22T03:12:48.175448Z",
     "shell.execute_reply": "2025-05-22T03:12:48.465062Z"
    }
   },
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "source": "# 5. Training & Validation",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## 5.1 Train and checkpoint",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import os\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader, random_split\nfrom torch.cuda.amp import autocast, GradScaler\nfrom torch.nn.functional import cross_entropy, mse_loss, cosine_similarity\nimport esm\nfrom tqdm import tqdm\nfrom torch.nn.utils.rnn import pad_sequence\n\n\nfor ep in range(1, TOTAL_EPOCHS+1):\n        # adjust LR and weights\n        if ep <= EPOCHS_PHASE1:\n            lr       = LR_PHASE1\n            w_ce, w_cos, w_mse, w_kl = CE_WEIGHT1, 0.0, 0.0, 0.0\n        else:\n            lr       = LR_PHASE2\n            w_ce, w_cos, w_mse, w_kl = CE_WEIGHT2, COS_WEIGHT, MSE_WEIGHT, KL_WEIGHT\n        for pg in vae.parameters():\n            pg.requires_grad = True\n        optimizer = optim.AdamW(vae.parameters(), lr=lr)\n\n        # train\n        stats = {'ce':0,'cos':0,'mse':0,'kl':0}\n        vae.train()\n        for x,mask in tqdm(train_loader, desc=f\"Train{ep}\"):\n            x,mask = x.to(device), mask.to(device)\n            optimizer.zero_grad()\n            with autocast():\n                logits, mu, logvar, h_enc, enc_mask = vae(x,mask)\n                # CE\n                ce = cross_entropy(\n                    logits.view(-1,logits.size(-1)),\n                    x.view(-1),\n                    ignore_index=PAD_IDX,\n                    label_smoothing=0.0\n                )\n                # teacher losses\n                with torch.no_grad():\n                    orig_h,_  = teacher(x)\n                recon_tokens = logits.argmax(-1)\n                recon_h,_  = teacher(recon_tokens)\n                cos_res = 1 - cosine_similarity(orig_h, recon_h, dim=-1)\n                cos = cos_res.masked_select(mask).mean()\n                mse_feat = (orig_h - recon_h).pow(2).mean(-1)\n                mse = mse_feat.masked_select(mask).mean()\n                # KL\n                kl = -0.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp())\n\n                loss = w_ce*torch.exp(ce) + w_cos*cos + w_mse*mse + w_kl*kl\n\n            scaler.scale(loss).backward()\n            scaler.step(optimizer); scaler.update()\n\n            stats['ce']  += ce.item()\n            stats['cos'] += cos.item()\n            stats['mse'] += mse.item()\n            stats['kl']  += kl.item()\n        for k in stats: stats[k] /= len(train_loader)\n\n        # validate\n        vstats = {'ce':0,'cos':0,'mse':0,'kl':0}\n        vae.eval()\n        with torch.no_grad():\n            for x,mask in val_loader:\n                x,mask = x.to(device), mask.to(device)\n                logits, mu, logvar, h_enc, enc_mask = vae(x,mask)\n                ce = cross_entropy(logits.view(-1,logits.size(-1)), x.view(-1), ignore_index=PAD_IDX, label_smoothing=0.0)\n                orig_h,_  = teacher(x)\n                recon_h,_ = teacher(logits.argmax(-1))\n                cos = (1 - cosine_similarity(orig_h, recon_h, dim=-1)).masked_select(mask).mean()\n                mse = ((orig_h - recon_h).pow(2).mean(-1)).masked_select(mask).mean()\n                kl  = -0.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp())\n\n                vstats['ce']  += ce.item()\n                vstats['cos'] += cos.item()\n                vstats['mse'] += mse.item()\n                vstats['kl']  += kl.item()\n        for k in vstats: vstats[k] /= len(val_loader)\n\n        print(\n            f\"Epoch {ep:2d} | \"\n            f\"Train CE={stats['ce']:.3f} COS={stats['cos']:.3f} MSE={stats['mse']:.3f} KL={stats['kl']:.3f} | \"\n            f\" Val CE={vstats['ce']:.3f} COS={vstats['cos']:.3f} MSE={vstats['mse']:.3f} KL={vstats['kl']:.3f}\"\n        )\n        if ep%10==0:\n            # ─── Checkpoint save ───\n            SAVE_PATH = f\"/kaggle/working/vae_epoch{ep:03d}.pt\"\n            model_to_save = vae.module if hasattr(vae, \"module\") else vae\n            torch.save({\n                \"epoch\":    ep,\n                \"model_sd\": model_to_save.state_dict(),\n                \"opt_sd\":   optimizer.state_dict(),\n                \"scaler_sd\": scaler.state_dict(),\n            }, SAVE_PATH)\n            print(f\"Saved checkpoint to {SAVE_PATH}\")",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "# 6. Load Saved VAE",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## 6.1 Model definition",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# ── Config ──\nMAX_LEN       = 512\nBATCH_SIZE    = 64\nLATENT_DIM    = 256\nEMB_DIM       = 256\nNUM_LAYERS    = 4\nNUM_HEADS     = 4\nFFN_DIM       = 512\nDROPOUT       = 0.3\n\nLR_PHASE1     = 5e-4    # higher LR for CE‐only warmup\nLR_PHASE2     = 1e-4    # later LR\n\nEPOCHS_PHASE1 = 100      # CE‐only for first 20 epochs\nTOTAL_EPOCHS  = 500\n\nCE_WEIGHT1    = 100.0    # CE weight during phase1\nCE_WEIGHT2    = 1.0     # CE weight afterwards\nKL_WEIGHT     = 0.1\nCOS_WEIGHT    = 5.0\nMSE_WEIGHT    = 5.0",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-05-21T04:41:52.929448Z",
     "iopub.execute_input": "2025-05-21T04:41:52.929745Z",
     "iopub.status.idle": "2025-05-21T04:41:52.934595Z",
     "shell.execute_reply.started": "2025-05-21T04:41:52.929723Z",
     "shell.execute_reply": "2025-05-21T04:41:52.933869Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "class ProteinDataset(Dataset):\n    def __init__(self, sequences, alphabet):\n        self.sequences = [seq[:MAX_LEN] for seq in sequences]\n        self.alphabet  = alphabet\n\n    def __len__(self):\n        return len(self.sequences)\n\n    def __getitem__(self, idx):\n        idxs = [self.alphabet.get_idx(c) for c in self.sequences[idx]]\n        return torch.tensor(idxs, dtype=torch.long)\n\ndef collate_fn(batch, pad_idx):\n    padded = pad_sequence(batch, batch_first=True, padding_value=pad_idx)\n    mask   = (padded != pad_idx)\n    return padded, mask\n    \nclass SmallTransformer(nn.Module):\n    def __init__(self, vocab_size, emb_dim, layers, heads, ffn_dim, max_len, pad_idx):\n        super().__init__()\n        self.emb = nn.Embedding(vocab_size, emb_dim, padding_idx=pad_idx)\n        self.pos = nn.Parameter(torch.zeros(1, max_len, emb_dim))\n        layer   = nn.TransformerEncoderLayer(\n            d_model=emb_dim, nhead=heads,\n            dim_feedforward=ffn_dim, batch_first=True,\n            activation='gelu', dropout=DROPOUT\n        )\n        self.enc = nn.TransformerEncoder(layer, layers)\n        self.ln  = nn.LayerNorm(emb_dim)\n\n    def forward(self, x):\n        mask = x != self.emb.padding_idx\n        h    = self.emb(x) + self.pos[:, :x.size(1), :]\n        h    = self.enc(h, src_key_padding_mask=~mask)\n        return self.ln(h), mask\n\nclass BigTransformer(SmallTransformer):\n    pass  # identical API\n\n# ── Single‐stage VAE ──\nclass VAETransformerDecoder(nn.Module):\n    def __init__(self, encoder, vocab_size, latent_dim=LATENT_DIM,\n                 emb_dim=EMB_DIM, num_layers=NUM_LAYERS, num_heads=NUM_HEADS,\n                 ffn_dim=FFN_DIM, max_len=MAX_LEN, pad_token=0, bos_token=1):\n        super().__init__()\n        self.encoder   = encoder\n        self.pad_token = pad_token\n        self.bos_token = bos_token\n\n        # latent heads\n        self.to_mu     = nn.Linear(emb_dim, latent_dim)\n        self.to_logvar = nn.Linear(emb_dim, latent_dim)\n        self.latent2emb= nn.Linear(latent_dim, emb_dim)\n\n        # decoder\n        self.dec_emb = nn.Embedding(vocab_size, emb_dim, padding_idx=pad_token)\n        self.dec_pos = nn.Parameter(torch.zeros(1, max_len, emb_dim))\n        layer = nn.TransformerDecoderLayer(\n            d_model=emb_dim, nhead=num_heads,\n            dim_feedforward=ffn_dim, dropout=DROPOUT,\n            batch_first=True\n        )\n        self.decoder = nn.TransformerDecoder(layer, num_layers)\n        self.out     = nn.Linear(emb_dim, vocab_size)\n\n    def forward(self, x, mask):\n        # encode\n        h_enc, enc_mask = self.encoder(x)\n        pooled = (h_enc * enc_mask.unsqueeze(-1)).sum(1) / enc_mask.sum(1, True)\n        mu, logvar = self.to_mu(pooled), self.to_logvar(pooled)\n        z = mu + torch.randn_like(mu) * torch.exp(0.5 * logvar)\n\n        # prepare decoder input\n        B, L = x.size()\n        dec_in = torch.full((B, L), self.bos_token, device=x.device, dtype=torch.long)\n        dec_in[:,1:] = x[:,:-1]\n        emb = self.dec_emb(dec_in) + self.dec_pos[:, :L, :]\n        z_emb = self.latent2emb(z).unsqueeze(1).expand(-1, L, -1)\n        emb = emb + z_emb\n\n        tgt_mask = nn.Transformer.generate_square_subsequent_mask(L).to(x.device)\n        h_dec = self.decoder(\n            tgt=emb,\n            memory=h_enc,\n            tgt_mask=tgt_mask,\n            tgt_key_padding_mask=~mask,\n            memory_key_padding_mask=~enc_mask\n        )\n        logits = self.out(h_dec)\n        return logits, mu, logvar, h_enc, enc_mask",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-05-21T04:41:55.308899Z",
     "iopub.execute_input": "2025-05-21T04:41:55.309165Z",
     "iopub.status.idle": "2025-05-21T04:41:55.321275Z",
     "shell.execute_reply.started": "2025-05-21T04:41:55.309144Z",
     "shell.execute_reply": "2025-05-21T04:41:55.320632Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## 6.2 load model",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# load alphabet & teacher\nimport esm\n\n_, alphabet = esm.pretrained.esm2_t33_650M_UR50D()\nPAD_IDX     = alphabet.get_idx('<pad>')\nBOS_IDX     = alphabet.get_idx('<cls>')\nenc = BigTransformer(\n        len(alphabet.all_toks), EMB_DIM, NUM_LAYERS, NUM_HEADS, FFN_DIM, MAX_LEN, PAD_IDX\n    ).to(device)\nvae = VAETransformerDecoder(\n        encoder=enc,\n        vocab_size=len(alphabet.all_toks),\n        latent_dim=LATENT_DIM,\n        emb_dim=EMB_DIM, num_layers=NUM_LAYERS, num_heads=NUM_HEADS,\n        ffn_dim=FFN_DIM, max_len=MAX_LEN,\n        pad_token=PAD_IDX, bos_token=BOS_IDX\n    ).to(device)\n#vae = nn.DataParallel(vae)\n# ── 0) Config ──\ndevice      = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nCHECKPOINT  = \"/kaggle/input/esms-vae/pytorch/default2/1/vae_epoch380.pt\"\nnoise_scale = 0.2   # tweak as needed\n\n# load alphabet (must match training)\n_, alphabet = esm.pretrained.esm2_t33_650M_UR50D()\n\n# your BOS (“begin sequence”) and PAD token IDs:\nBOS_IDX     = alphabet.get_idx('<cls>')\nPAD_IDX     = alphabet.get_idx('<pad>')\n\n# ── 1) Load model + checkpoint ──\n# assume `vae` is already defined (your VAETransformerDecoder wrapped in DataParallel)\nckpt = torch.load(CHECKPOINT, map_location=device)\nmodel = vae.module if hasattr(vae, \"module\") else vae\nmodel.load_state_dict(ckpt[\"model_sd\"])\nmodel.to(device).eval()",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-05-21T04:43:55.658205Z",
     "iopub.execute_input": "2025-05-21T04:43:55.658872Z",
     "iopub.status.idle": "2025-05-21T04:44:08.856146Z",
     "shell.execute_reply.started": "2025-05-21T04:43:55.658848Z",
     "shell.execute_reply": "2025-05-21T04:44:08.855572Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## 6.3 helper function",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# ── Helpers ──\ndef reconstruction_accuracy(orig: str, recon: str) -> float:\n    assert len(orig) == len(recon), \"Lengths must match\"\n    return sum(o == r for o, r in zip(orig, recon)) / len(orig) * 100.0\n\ndef decode_batch(id_seqs, alphabet, pad_idx):\n    strs = []\n    for seq in id_seqs.cpu().tolist():\n        chars = []\n        for idx in seq:\n            if idx == pad_idx:\n                break\n            chars.append(alphabet.get_tok(idx))\n        strs.append(\"\".join(chars))\n    return strs",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-05-21T04:44:29.813602Z",
     "iopub.execute_input": "2025-05-21T04:44:29.814106Z",
     "iopub.status.idle": "2025-05-21T04:44:29.818988Z",
     "shell.execute_reply.started": "2025-05-21T04:44:29.814082Z",
     "shell.execute_reply": "2025-05-21T04:44:29.818431Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## 6.4 load data",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "\n# ── 2) Prepare a batch x, mask ──\n# Example using a test DataLoader of token-ID tensors:\ndef collate_fn(batch, pad_idx):\n    padded = pad_sequence(batch, batch_first=True, padding_value=pad_idx)\n    mask   = (padded != pad_idx)\n    return padded, mask\nds        = ProteinDataset(sequences, alphabet)\nt,v,s     = int(0.8*len(ds)), int(0.1*len(ds)), len(ds) - int(0.9*len(ds))\ntrain_ds, val_ds, test_ds = random_split(ds, [t,v,s])\n# Suppose you have a `test_dataset` yielding LongTensor sequences of token‐IDs:\ntest_loader = DataLoader(\n    test_ds,\n    batch_size=64,\n    shuffle=False,\n    collate_fn=lambda b: collate_fn(b, PAD_IDX)\n)\n# grab one batch\nx, mask = next(iter(test_loader))\nx, mask = x.to(device), mask.to(device)",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-05-21T05:07:01.873012Z",
     "iopub.execute_input": "2025-05-21T05:07:01.873687Z",
     "iopub.status.idle": "2025-05-21T05:07:01.888849Z",
     "shell.execute_reply.started": "2025-05-21T05:07:01.873662Z",
     "shell.execute_reply": "2025-05-21T05:07:01.888192Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## 6.4 Latent vector space test",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# ── 3) Forward through encoder ──\nwith torch.no_grad():\n    logits_clean, mu, logvar, h_enc, enc_mask = model(x, mask)\n\n    # compute z and noisy z\n    std = torch.exp(0.5 * logvar)\n    eps = torch.randn_like(std)\n    z   = mu + std * eps\n\n    eps2    = torch.randn_like(std)\n    z_noisy = z + noise_scale * eps2\n\n    # diagnostics\n    delta = (z_noisy - z).view(z.size(0), -1).norm(dim=1)\n    print(f\"│z_noisy − z│ mean: {delta.mean().item():.4f}, std: {delta.std().item():.4f}\")\n\n    z_emb_clean = model.latent2emb(z)\n    emb_norm    = z_emb_clean.view(z_emb_clean.size(0), -1).norm(dim=1)\n    print(f\"‖latent2emb(z)‖ mean: {emb_norm.mean().item():.4f}\")\n\n# ── 4) Decode clean + latent‐noise versions ──\ndef decode_from(z_latent):\n    B, L = x.size()\n    dec_in = torch.full((B, L), BOS_IDX, device=device, dtype=torch.long)\n    dec_in[:,1:] = x[:,:-1]\n    emb       = model.dec_emb(dec_in) + model.dec_pos[:, :L, :]\n    z_emb     = model.latent2emb(z_latent).unsqueeze(1).expand(-1, L, -1)\n    dec_input = emb + z_emb\n\n    tgt_mask  = nn.Transformer.generate_square_subsequent_mask(L).to(device)\n    h_dec = model.decoder(\n        tgt=dec_input,\n        memory=h_enc,\n        tgt_mask=tgt_mask,\n        tgt_key_padding_mask=~mask,\n        memory_key_padding_mask=~enc_mask\n    )\n    logits = model.out(h_dec)\n    return logits.argmax(-1)\n\nrecon_clean = decode_from(z)\nrecon_noisy = decode_from(z_noisy)",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-05-21T04:44:42.509605Z",
     "iopub.execute_input": "2025-05-21T04:44:42.509891Z",
     "iopub.status.idle": "2025-05-21T04:44:43.811890Z",
     "shell.execute_reply.started": "2025-05-21T04:44:42.509870Z",
     "shell.execute_reply": "2025-05-21T04:44:43.811227Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# 한 배치의 mu, logvar를 뽑아서 분포를 시각화\nimport matplotlib.pyplot as plt\nimport numpy as np\nmus = mu.detach().cpu().numpy().flatten()\nlogvars = logvar.detach().cpu().numpy().flatten()\nplt.hist(mus, bins=50); plt.title(\"mu distribution\"); plt.show()\nplt.hist(np.exp(0.5*logvars), bins=50); plt.title(\"sigma distribution\"); plt.show()\n",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-05-21T04:46:18.392867Z",
     "iopub.execute_input": "2025-05-21T04:46:18.393716Z",
     "iopub.status.idle": "2025-05-21T04:46:18.832673Z",
     "shell.execute_reply.started": "2025-05-21T04:46:18.393690Z",
     "shell.execute_reply": "2025-05-21T04:46:18.831995Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## 6.5 Posterior Collapse Check",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# ── 2) Posterior Collapse Check (KL per dimension) ──\nprint(\"\\n## Posterior Collapse Check\")\nwith torch.no_grad():\n    all_mu, all_logvar = [], []\n    for x, mask in test_loader:\n        x, mask = x.to(device), mask.to(device)\n        _, mu, logvar, _, _ = model(x, mask)\n        all_mu.append(mu); all_logvar.append(logvar)\n    mu = torch.cat(all_mu, dim=0)\n    logvar = torch.cat(all_logvar, dim=0)\n    kl_per_dim = (0.5 * (mu.pow(2) + logvar.exp() - 1 - logvar)).mean(0)\n    print(\"KL per dimension:\", kl_per_dim.cpu().numpy())",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-05-21T05:07:20.103274Z",
     "iopub.execute_input": "2025-05-21T05:07:20.103879Z",
     "iopub.status.idle": "2025-05-21T05:07:24.234340Z",
     "shell.execute_reply.started": "2025-05-21T05:07:20.103857Z",
     "shell.execute_reply": "2025-05-21T05:07:24.233654Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## 6.6 Novel generation",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import torch\nimport torch.nn.functional as F\nimport numpy as np\nfrom tqdm import tqdm, trange\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# DataParallel 해제한 VAE 모듈\nvae_module = vae \ndec_emb     = vae_module.dec_emb\ndec_pos     = vae_module.dec_pos\nlatent2emb  = vae_module.latent2emb\ndecoder     = vae_module.decoder\nout_proj    = vae_module.out\n\n# 상수\nMAX_LEN    = 512\nLATENT_DIM = 256\nPAD_IDX    = PAD_IDX\nBOS_IDX    = BOS_IDX\nEMB_DIM    = dec_emb.embedding_dim\n\ndef generate_from_z(z, max_len=MAX_LEN):\n    batch = z.size(0)\n    # 1) 초기 디코더 입력 (BOS + PAD)\n    generated = torch.full((batch, max_len), PAD_IDX, device=device, dtype=torch.long)\n    generated[:, 0] = BOS_IDX\n\n    # 2) causal mask\n    tgt_mask = torch.triu(torch.full((max_len, max_len), float('-inf')),\n                          diagonal=1).to(device)\n    # 3) dummy memory\n    memory = torch.zeros(batch, max_len, EMB_DIM, device=device)\n\n    with torch.no_grad():\n        for t in trange(1, max_len):\n            # 토큰+포지셔널+latent 임베딩\n            tok_emb = dec_emb(generated[:, :t])                          # (B, t, E)\n            pos_emb = dec_pos[:, :t, :]                                  # (1, t, E)\n            z_emb   = latent2emb(z).unsqueeze(1).expand(-1, t, -1)        # (B, t, E)\n            tgt     = tok_emb + pos_emb + z_emb                          # (B, t, E)\n\n            # 디코더 호출\n            dec_out = decoder(\n                tgt=tgt,\n                memory=memory,\n                tgt_mask=tgt_mask[:t, :t],\n                memory_key_padding_mask=None,\n                tgt_key_padding_mask=None\n            )  # (B, t, E)\n\n            # 다음 토큰 예측\n            logits     = out_proj(dec_out)                               # (B, t, V)\n            next_token = logits[:, -1].argmax(-1)                        # (B,)\n            generated[:, t] = next_token\n\n            # 모두 PAD면 종료\n            if (next_token == PAD_IDX).all():\n                break\n\n    # 4) 인덱스를 시퀀스로 변환\n    out_seqs = []\n    for seq in generated.cpu().tolist():\n        toks = [alphabet.all_toks[i] for i in seq if i not in (PAD_IDX, BOS_IDX)]\n        out_seqs.append(\"\".join(toks))\n\n    return out_seqs\n\n# -- 사용 예시 및 Novelty 계산 --\nn = 1000\nz_rand      = torch.randn(n, LATENT_DIM, device=device)\ngen_seqs    = generate_from_z(z_rand, max_len=MAX_LEN)\n\ndef seq_identity(a, b):\n    L = max(len(a), len(b))\n    a2, b2 = a.ljust(L, '-'), b.ljust(L, '-')\n    return sum(x==y for x,y in zip(a2, b2)) / L\n\nmax_id = []\nfor s in tqdm(gen_seqs, desc=\"Compute novelty\"):\n    ids = [seq_identity(s, t) for t in sequences]\n    max_id.append(max(ids))\n\nmax_id = np.array(max_id)\nprint(\"Percent ≤30% identity:\", np.mean(max_id <= 0.30)*100)\nprint(\"Median identity:       \", np.median(max_id))\nprint(\"90th percentile:       \", np.percentile(max_id, 90))\nprint(\"Max identity:          \", max_id.max())",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-05-21T05:18:21.071761Z",
     "iopub.execute_input": "2025-05-21T05:18:21.072462Z",
     "iopub.status.idle": "2025-05-21T05:37:28.732408Z",
     "shell.execute_reply.started": "2025-05-21T05:18:21.072435Z",
     "shell.execute_reply": "2025-05-21T05:37:28.731827Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## 6.7 Reconstruction test",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import torch\n\n# 1) 체크포인트 로드\nBOS_IDX     = alphabet.get_idx('<cls>')\nPAD_IDX     = alphabet.get_idx('<pad>')\nCHECKPOINT_PATH = \"/kaggle/input/esms-vae/pytorch/default2/1/vae_epoch380.pt\"  # 가장 최신 파일 경로로 설정\nckpt = torch.load(CHECKPOINT_PATH, map_location=device)\nmodel_to_save = vae.module if hasattr(vae, \"module\") else vae\nmodel_to_save.load_state_dict(ckpt[\"model_sd\"])\nmodel_to_save.eval()\nds        = ProteinDataset(sequences, alphabet)\ntrain_ds, test_ds = random_split(ds, [1000000-100000,100000])\n\ntest_loader   = DataLoader(test_ds,   BATCH_SIZE, shuffle=False,\n                              collate_fn=lambda b: collate_fn(b, PAD_IDX))\ndef reconstruction_accuracy(orig: str, recon: str) -> float:\n    \"\"\"\n    두 시퀀스(orig, recon)의 길이는 동일하다고 가정.\n    정확히 일치하는 토큰의 비율을 반환.\n    \"\"\"\n    assert len(orig) == len(recon), \"Original and reconstructed must have same length\"\n    matches = sum(o == r for o, r in zip(orig, recon))\n    return matches / len(orig) * 100.0  # percentage\n# 2) 검증 세트에서 재구성율 계산\ncorrect, total = 0, 0\nwith torch.no_grad():\n    for x, mask in test_loader:\n        x, mask = x.to(device), mask.to(device)\n        logits, *_ = vae(x, mask)  # logits만 필요\n        preds = logits.argmax(dim=-1)\n        correct += ((preds == x) & mask).sum().item()\n        total   += mask.sum().item()\n\nrecon_acc = correct / total\nprint(f\"Final Reconstruction Accuracy: {recon_acc*100:.5f}%\")\n\n# 2) 검증 세트에서 5개 시퀀스 재구성 출력\nfrom torch.nn.functional import pad\n\nn_show = 5\nshown = 0\n\nwith torch.no_grad():\n    for x, mask in test_loader:\n        x, mask = x.to(device), mask.to(device)\n        logits, *_ = vae(x, mask)\n        preds = logits.argmax(dim=-1)\n\n        # 배치 내 각 시퀀스에 대해\n        for orig_ids, pred_ids, m in zip(x, preds, mask):\n            # mask를 이용해 실제 토큰 길이만 추출\n            length = m.sum().item()\n            orig_seq = [alphabet.all_toks[i] for i in orig_ids[:length].tolist()]\n            pred_seq = [alphabet.all_toks[i] for i in pred_ids[:length].tolist()]\n            \n            acc=reconstruction_accuracy(orig_seq, pred_seq)\n\n            print(f\"=== Example {shown+1} ===\")\n            print(f'reconstruction accuracy: {acc}')\n\n            print(\"Original:       \", \"\".join(orig_seq))\n            print(\"Reconstructed:  \", \"\".join(pred_seq))\n            print()\n\n            shown += 1\n            if shown >= n_show:\n                break\n        if shown >= n_show:\n            break",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-05-22T03:12:58.776778Z",
     "iopub.execute_input": "2025-05-22T03:12:58.777537Z",
     "iopub.status.idle": "2025-05-22T03:15:51.419486Z",
     "shell.execute_reply.started": "2025-05-22T03:12:58.777502Z",
     "shell.execute_reply": "2025-05-22T03:15:51.418688Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "text": "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:508: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. We recommend specifying layout=torch.jagged when constructing a nested tensor, as this layout receives active development, has better operator coverage, and works with torch.compile. (Triggered internally at /pytorch/aten/src/ATen/NestedTensorImpl.cpp:178.)\n  output = torch._nested_tensor_from_mask(\n/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py:5962: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py:5962: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n  warnings.warn(\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "Final Reconstruction Accuracy: 97.17987%\n=== Example 1 ===\nreconstruction accuracy: 100.0\nOriginal:        MNLSMTDRDNATATSDSSRTACSVSRAAGPAVQLRIGRLRRTIGHRDHVRPGRDPVGEQLREHRAGDQAAFPGR\nReconstructed:   MNLSMTDRDNATATSDSSRTACSVSRAAGPAVQLRIGRLRRTIGHRDHVRPGRDPVGEQLREHRAGDQAAFPGR\n\n=== Example 2 ===\nreconstruction accuracy: 98.82352941176471\nOriginal:        MIGYGNEEFGYKLWDPEKQKIVRSRDIVFHEHETIKDMEKNVVSTKLTYEGNLDEEIFMEQLEGFKVKGKENMVCKLKKSMYGLK\nReconstructed:   MIGYGNEEFGYKLWDPEKQKIVRSRDIVFHEHETIKDMEKNVVSTKLTYEGNLDEEIFMEQLEGFKVKGKENMVCKLKSSMYGLK\n\n=== Example 3 ===\nreconstruction accuracy: 96.98795180722891\nOriginal:        MRVVRWLDTGLNSLNFLLHQISNLILMLIMFLTTFDVIGRALFNHSITGAYELTELGSAIVIFFTLAVTHKYKEHVAVGFLVDKLSAKKKAMIEGLVDLFIFVLILIMSFQLINEAMRLMERGTTTTDLGLPIYTFILIVSIGSFIFAFVALANGIKSMIEAVKKS\nReconstructed:   MRVVRWLDTGLNSLNFLLHQISNLILMLIMFLTTFDVIGRALFNHSITGAYELTELGSAIVIFTTLAVTHKYKEHVAVGFLVDKLSAKKAAMIEGLVDLFIFVLILMMSFQLINEAMRLMERGTTTDDLGLPIYTFILIVSIGSFIFAVVALANGIKSMIEAVKKS\n\n=== Example 4 ===\nreconstruction accuracy: 98.24561403508771\nOriginal:        MYMIDRGVAIIRPRQPFVDWINGLPDADMKVNLEDVRSDCLVVLISPFVSEADAMEEIAELYEDIFKTELTDWCPEQKWWPAKRDLETFRQWFDIEVHALVADPFDEEIRKEPY\nReconstructed:   MYMIDRGVAIIRPRQPFVDWINGLPDADMKVNLEDVRSDCLVVLISPFVSEADAMEIIAELYEDIFKTELTDWCPEQKWPPAKRDLETFRQWFDIEVHALVADPFDEEIRKEPY\n\n=== Example 5 ===\nreconstruction accuracy: 96.63865546218487\nOriginal:        MKIVVKNKNISRKDFEKHFQLLFHSNLLSNNNISYSITETKLRSVDSSVLMVLVSSSGVVLTTLISWLFKLAITKGSQKIVIQGKNGKKIEVPAGTSEKDIDKFLEKASKLSVDSIFIE\nReconstructed:   MKIVVKNKNISRKDFEKHFQLLFHSNLLSNNIISYSITETKLRSVDSSVLMVLVSSGGVLLTLLISWLFKLAITKGSQKIVIQGKNGKKIEVPAGTSEKDIDKFLEKASKLSVDSIFIE\n\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "source": "## 7. Evaluate Loaded VAE",
   "metadata": {}
  }
 ]
}
