# ESMS-VAE

ESMS-VAE is a simple implementation of a Variational Autoencoder (VAE) for protein sequences. The notebooks use embeddings from the `fair-esm` package and demonstrate how to train and evaluate the model on a subset of the UniRef50 dataset.

## Repository Contents

- `esms-vae-structured.ipynb` – step-by-step training and evaluation workflow.
- `gfp-cluster.ipynb` – example notebook for clustering latent vectors with `KMeans`.
- `gfp-regressor.ipynb` – example notebook showing a regression task using latent features.
- `vae_epoch380.pt` – pretrained model checkpoint produced by the training notebook.

## Getting Started

1. Install dependencies:
   ```bash
   pip install torch fair-esm biopython scikit-learn
   ```
2. Open `esms-vae-structured.ipynb` in Jupyter and run the cells to download the data, train the VAE, and save a checkpoint.
3. To reuse the provided checkpoint, start from the **Load Saved VAE** section of the notebook.

The notebooks expect access to example datasets from Kaggle (e.g. `uniref50_subsample.fasta`). Adjust the paths in the notebooks if your data is stored elsewhere.

## Using the Notebooks

After training, you can explore the latent space with the clustering and regression notebooks. Each notebook assumes latent vectors have been generated by the main training notebook.

- `gfp-cluster.ipynb` clusters sequences using `KMeans`.
- `gfp-regressor.ipynb` fits a simple regression model to latent features.

## Pretrained Model

The file `vae_epoch380.pt` contains weights trained for 380 epochs. Load this checkpoint in the main notebook to reconstruct sequences or generate new examples.

---

This repository is intended as a lightweight reference for experiments with protein VAEs and ESM embeddings. Feel free to adapt the notebooks for your own datasets or downstream tasks.
