# ESMS-VAE

ESMS-VAE is a simple implementation of a Variational Autoencoder (VAE) for protein sequences. The notebooks use embeddings from the `fair-esm` package and demonstrate how to train and evaluate the model on a subset of the UniRef50 dataset.

## Repository Contents

- `esms-vae-structured.ipynb` – step-by-step training and evaluation workflow.
- `gfp-cluster.ipynb` – example notebook for clustering latent vectors with `KMeans`.
- `gfp-regressor.ipynb` – example notebook showing a regression task using latent features.
- `vae_epoch380.pt` – pretrained model checkpoint produced by the training notebook.

## Getting Started

1. Install dependencies (or use `requirements.txt`):
   ```bash
   pip install -r requirements.txt
   ```
2. Open `esms-vae-structured.ipynb` in Jupyter and run the cells to download the data, train the VAE, and save a checkpoint.
3. To reuse the provided checkpoint, start from the **Load Saved VAE** section of the notebook.

The notebooks expect access to example datasets from Kaggle (e.g. `uniref50_subsample.fasta`). Adjust the paths in the notebooks if your data is stored elsewhere.

## Using the Notebooks

After training, you can explore the latent space with the clustering and regression notebooks. Each notebook assumes latent vectors have been generated by the main training notebook.

- `gfp-cluster.ipynb` clusters sequences using `KMeans`.
- `gfp-regressor.ipynb` fits a simple regression model to latent features.

## Pretrained Model

The file `vae_epoch380.pt` contains weights trained for 380 epochs. Load this checkpoint in the main notebook to reconstruct sequences or generate new examples.

## Library Usage Example

You can interact with the model programmatically using the `vae_module` package.
The snippet below demonstrates how to load the pretrained checkpoint and encode a sequence:

```python
from vae_module import Tokenizer, Config, load_vae, encode

cfg = Config(model_path="vae_epoch380.pt")
tokenizer = Tokenizer.from_esm()
model = load_vae(cfg, vocab_size=len(tokenizer.vocab),
                 pad_idx=tokenizer.pad_idx, bos_idx=tokenizer.bos_idx)

z = encode(model, "MKTFFVLLL", tokenizer, cfg.max_len)
```

The resulting tensor `z` contains the latent representation of the sequence.

## Building Documentation

The `vae_module/docs` directory includes a minimal Sphinx configuration. After
installing the dependencies you can generate HTML documentation with:

```bash
sphinx-build -b html vae_module/docs docs
```

---

This repository is intended as a lightweight reference for experiments with protein VAEs and ESM embeddings. Feel free to adapt the notebooks for your own datasets or downstream tasks.
